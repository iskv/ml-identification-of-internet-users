{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: “Identification Of Internet Users”\n",
    "\n",
    "### The problem: for a sequence of web sites visited in a row by the same person, identify that person.\n",
    "\n",
    "The data is taken from [the article](http://ceur-ws.org/Vol-1703/paper12.pdf) \"A Tool for Classification of Sequential Data\".\n",
    "\n",
    "For each user, a csv file is created with the name user\\*\\*\\*\\*. csv (where instead of asterisks – 4 digits corresponding to the user's ID), and site visits are recorded in the following format:\n",
    "\n",
    "<center>*timestamp, visited website*</center>\n",
    "\n",
    "In the project, not all operations can be performed in a reasonable amount of time (for example, we can cross-validate 100 combinations of random forest parameters based on this data), so we will use 2 samples in parallel: 10 users and 150. We will write and debug code for 10 users, and we will have a working version for 150.\n",
    "\n",
    "The data is arranged as follows:\n",
    "\n",
    " - The 10users directory contains 10 csv files with the name \"user[USER_ID].csv\", where [USER_ID] is the user ID;\n",
    " - Similarly, for the 150users directory, there are 150 files;\n",
    "\n",
    "# Part 1. Preparing data for analysis and building models\n",
    "\n",
    "The first part of the project is devoted to preparing data for further descriptive analysis and building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of a file with data about web pages visited by the user:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'capstone_user_identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user31_data = pd.read_csv(os.path.join(PATH_TO_DATA, '10users/user0031.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-15 08:12:07</td>\n",
       "      <td>fpdownload2.macromedia.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-15 08:12:17</td>\n",
       "      <td>www.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15 08:12:18</td>\n",
       "      <td>www.laposte.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        site\n",
       "0  2013-11-15 08:12:07  fpdownload2.macromedia.com\n",
       "1  2013-11-15 08:12:17                 laposte.net\n",
       "2  2013-11-15 08:12:17             www.laposte.net\n",
       "3  2013-11-15 08:12:17              www.google.com\n",
       "4  2013-11-15 08:12:18             www.laposte.net"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's formulate a classification problem: identify a user by a session of 10 consecutive visited sites. The object in this task will be a session of 10 sites visited sequentially by the same user, and the attributes will be the indexes of these 10 sites (a little later, a \"bag\" of sites, the Bag of Words approach, will appear here). The target class will be the user id.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing a training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\d{4}'\n",
    "\n",
    "def prepare_train_set(path_to_csv_files, session_length=10):\n",
    "    '''\n",
    "    The function returns a table of sessions and a frequency dictionary of site displays in id.\n",
    "    \n",
    "    Parametrs:\n",
    "    - path_to_csv_files - the path to the directory with the files that are relevant to users.\n",
    "    - session_length - session length.\n",
    "    \n",
    "    Return:\n",
    "    - a table of user sessions and a frequency dictionary of site displays in the id \n",
    "    view: {'site_string': [site_id, site_freq]}.\n",
    "    '''\n",
    "    \n",
    "    out_dict = dict()    \n",
    "    out_df = pd.DataFrame()\n",
    "    \n",
    "    for usr in glob(path_to_csv_files + '\\\\user*.csv'):        \n",
    "        sites_list = pd.read_csv(usr).site.to_list() \n",
    "        \n",
    "        # Creating a dictionary\n",
    "        for site in sites_list:\n",
    "            if site in out_dict:\n",
    "                out_dict[site][1] += 1\n",
    "            else:\n",
    "                out_dict[site] = [len(out_dict) + 1, 1]        \n",
    "        \n",
    "        # Converting 1D to 2D, adding zeros to the list\n",
    "        while len(sites_list) % session_length:\n",
    "            sites_list.append(0)              \n",
    "        \n",
    "        sites_list = np.reshape(sites_list, (len(sites_list) // session_length, session_length)).tolist()        \n",
    "        \n",
    "        # Add information about the user\n",
    "        for i in range(len(sites_list)):            \n",
    "            sites_list[i].append(int(*re.findall(pattern, usr)))         \n",
    "        \n",
    "        # Adding data about user sessions to the shared DataFrame\n",
    "        out_df = pd.concat([out_df, pd.DataFrame(sites_list)], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    # Setting the smallest indexes for the most frequent sites    \n",
    "    for i, site in enumerate(sorted(out_dict, key=lambda x: out_dict[x][1], reverse=True)):\n",
    "        out_dict[site][0] = i + 1   \n",
    "    \n",
    "    # Replacing sites with indexes\n",
    "    out_df.columns = ['site' + str(i + 1) for i in range(session_length)] + ['user_id'] \n",
    "    out_df = out_df.applymap(lambda x: out_dict[x][0] if x in out_dict else int(x))\n",
    "    \n",
    "    return out_df, out_dict     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of the function implementation:\n",
    "* When assigning indexes to sites, the smallest description principle is used (smaller indexes are assigned to frequently encountered sites)\n",
    "* Entity recognition is not taken into account, i.e. http://www.google.com and www.google.com they are considered different sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10users, site_freq_10users = prepare_train_set(os.path.join(PATH_TO_DATA, '10users'), session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>574</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>203</td>\n",
       "      <td>133</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415</td>\n",
       "      <td>193</td>\n",
       "      <td>674</td>\n",
       "      <td>254</td>\n",
       "      <td>133</td>\n",
       "      <td>31</td>\n",
       "      <td>393</td>\n",
       "      <td>3305</td>\n",
       "      <td>217</td>\n",
       "      <td>55</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>293</td>\n",
       "      <td>415</td>\n",
       "      <td>333</td>\n",
       "      <td>897</td>\n",
       "      <td>55</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>473</td>\n",
       "      <td>3306</td>\n",
       "      <td>473</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>937</td>\n",
       "      <td>199</td>\n",
       "      <td>123</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>342</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>3307</td>\n",
       "      <td>258</td>\n",
       "      <td>211</td>\n",
       "      <td>3308</td>\n",
       "      <td>2086</td>\n",
       "      <td>675</td>\n",
       "      <td>2086</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site1  site2  site3  site4  site5  site6  site7  site8  site9  site10  \\\n",
       "0    192    574    133      3    133    133      3    133    203     133   \n",
       "1    415    193    674    254    133     31    393   3305    217      55   \n",
       "2     55      3     55     55      5    293    415    333    897      55   \n",
       "3    473   3306    473     55     55     55     55    937    199     123   \n",
       "4    342     55      5   3307    258    211   3308   2086    675    2086   \n",
       "\n",
       "   user_id  \n",
       "0       31  \n",
       "1       31  \n",
       "2       31  \n",
       "3       31  \n",
       "4       31  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_10users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fpdownload2.macromedia.com': [192, 88],\n",
       " 'laposte.net': [574, 23],\n",
       " 'www.laposte.net': [133, 119],\n",
       " 'www.google.com': [3, 5441],\n",
       " 'match.rtbidder.net': [203, 84],\n",
       " 'x2.vindicosuite.com': [415, 37],\n",
       " 'rp.gwallet.com': [193, 88],\n",
       " 'pool-eu-ie.creative-serving.com': [674, 18],\n",
       " 'dl.javafx.com': [254, 65],\n",
       " 'ajax.googleapis.com': [31, 711],\n",
       " 'api.dailymotion.com': [393, 40],\n",
       " 'i1-js-14-3-01-11074-266576264-i.init.cedexis-radar.net': [3305, 1],\n",
       " 'limelight.cedexis.com': [217, 78],\n",
       " 'webmail.laposte.net': [55, 399],\n",
       " 'www.facebook.com': [5, 4141],\n",
       " 'rubicon-match.dotomi.com': [293, 55],\n",
       " 'pr.ybp.yahoo.com': [333, 47],\n",
       " 'dtm.ccs.com': [897, 12],\n",
       " 'b12.myspace.com': [473, 31],\n",
       " 'i1-js-14-3-01-11074-845302217-i.init.cedexis-radar.net': [3306, 1],\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_freq_10users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sessions from 10 sites in a sample with 10 users: 14061\n",
      "Number of unique sites in a web sample of 10 users: 4913\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique sessions from 10 sites in a sample with 10 users:', len(train_data_10users))\n",
    "print('Number of unique sites in a web sample of 10 users:', len(site_freq_10users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_150users, site_freq_150users = prepare_train_set(os.path.join(PATH_TO_DATA, '150users'), session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sessions from 10 sites in a sample with 150 users: 137019\n",
      "Number of unique sites in a web sample of 150 users 27797\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique sessions from 10 sites in a sample with 150 users:', len(train_data_150users))\n",
    "print('Number of unique sites in a web sample of 150 users', len(site_freq_150users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.google.fr',\n",
       " 'www.google.com',\n",
       " 'www.facebook.com',\n",
       " 'apis.google.com',\n",
       " 's.youtube.com',\n",
       " 'clients1.google.com',\n",
       " 'mail.google.com',\n",
       " 'plus.google.com',\n",
       " 'safebrowsing-cache.google.com',\n",
       " 'www.youtube.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most popular sites visited by 150 users\n",
    "sorted(site_freq_150users, key=lambda x: site_freq_150users[x][1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analysis, we will save the received DataFrame objects to csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10users.to_csv(os.path.join(PATH_TO_DATA, 'train_data_10users.csv'), index_label='session_id', float_format='%d')\n",
    "train_data_150users.to_csv(os.path.join(PATH_TO_DATA, 'train_data_150users.csv'), index_label='session_id', float_format='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with sparse data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting features *site1*,..., *site10* do not make sense as features in the classification problem. But if you use the idea of a bag of words from text analysis – this is another matter. We will create new matrices in which rows will correspond to sessions from 10 sites, and columns will correspond to site indexes. At the intersection of the $i$ row and the $j$ column, the number $n_{ij}$ will be set – the number of times the $j$ site met in the $i$session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matr_gen(X):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "\n",
    "    for session in X:\n",
    "        for site in session:\n",
    "            if site == 0: continue # site with id = 0 is not taken into account\n",
    "            indices.append(site-1) # for numbering with id = 0 (not id = 1), i.e. deleting column 0\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "        \n",
    "    return csr_matrix((data, indices, indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_10users, y_10users = train_data_10users.iloc[:, :-1].values, train_data_10users.iloc[:, -1].values\n",
    "X_150users, y_150users = train_data_150users.iloc[:, :-1].values, train_data_150users.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse_10users = sparse_matr_gen(X_10users)\n",
    "X_sparse_150users = sparse_matr_gen(X_150users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We save these sparse matrices and vectors *y_10users, y_150users* - target values (user id) for samples of 10 and 150 users using [pickle](https://docs.python.org/2/library/pickle.html). We will also save the frequency dictionaries of sites for 10 and 150 users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_10users.pkl'), 'wb') as X10_pkl:\n",
    "    pickle.dump(X_sparse_10users, X10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_10users.pkl'), 'wb') as y10_pkl:\n",
    "    pickle.dump(y_10users, y10_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'wb') as X150_pkl:\n",
    "    pickle.dump(X_sparse_150users, X150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'wb') as y150_pkl:\n",
    "    pickle.dump(y_150users, y150_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_10users.pkl'), 'wb') as site_freq_10users_pkl:\n",
    "    pickle.dump(site_freq_10users, site_freq_10users_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq_150users.pkl'), 'wb') as site_freq_150users_pkl:\n",
    "    pickle.dump(site_freq_150users, site_freq_150users_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "* Created session tables for samples of 10 and 150 users.\n",
    "* Converted the original attributes to counter attributes that make sense for the classification task.\n",
    "\n",
    "In the next part of the project (part2_analysis_hypotheses.ipynb), we will continue to prepare data, as well as test some hypotheses related to our observations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
